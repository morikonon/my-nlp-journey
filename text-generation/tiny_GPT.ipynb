{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import necessary libraries**"
      ],
      "metadata": {
        "id": "2nThQfjDVaZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "from torch.cuda import is_available"
      ],
      "metadata": {
        "id": "MveMdLtTVdpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if is_available() else torch.device(\"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0Cn8x5XkZUS",
        "outputId": "3850f559-be97-439d-99cf-9b65bb70dc63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating a Transformer from scratch**"
      ],
      "metadata": {
        "id": "DhS1AEzDWMu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Head\n",
        "2. MultiHead\n",
        "3. FeedForward\n",
        "4. Block\n",
        "5. GPTmodel\n"
      ],
      "metadata": {
        "id": "ZtWKvJkdWX-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, embedding_dim, head_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    #\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.head_dim = head_dim\n",
        "\n",
        "    # our projection matrices\n",
        "    self.q_projection = nn.Linear(embedding_dim, head_dim, bias=False)\n",
        "    self.k_projection = nn.Linear(embedding_dim, head_dim, bias=False)\n",
        "    self.v_projection = nn.Linear(embedding_dim, head_dim, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size() # initial dimension: Batch, Text, Embedding\n",
        "\n",
        "    # get Query, Key and Value matrices\n",
        "    Q = self.q_projection(x)\n",
        "    K = self.k_projection(x)\n",
        "    V = self.v_projection(x)\n",
        "\n",
        "    # get context by multiplaying Query and Key\n",
        "    # we transpose K to get (Batch, Text, Embedding) @ (Batch, Embedding, Text) -> (Batch, Text, Text)\n",
        "    context = (Q @ K.transpose(-2, -1) ) / (self.head_dim ** 0.5)\n",
        "\n",
        "    # we create a causual attention. it needs when we do a generative model\n",
        "    tril_ = torch.tril(torch.ones(T, T, device=x.device)) == 0\n",
        "    masked_context = context.masked_fill(tril_, float('-inf'))\n",
        "\n",
        "    # compute probabilies\n",
        "    probs_context = F.softmax(masked_context, dim=-1)\n",
        "\n",
        "    # compute attention\n",
        "    attention = probs_context @ V\n",
        "\n",
        "    return attention"
      ],
      "metadata": {
        "id": "y_HpzYK4WTtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedding_dim, head_dim, num_heads):\n",
        "    super().__init__()\n",
        "    assert embedding_dim / head_dim == num_heads, \"embedding_dim must be divided head_dim\"\n",
        "\n",
        "    self.heads = nn.ModuleList([Head(embedding_dim, head_dim) for _ in range(num_heads)])\n",
        "    self.projection = nn.Linear(num_heads * head_dim, embedding_dim)\n",
        "    self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    x = self.projection(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "q1_qU3AqZNLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Sequential(\n",
        "        nn.Linear(embedding_dim, embedding_dim * 4),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(embedding_dim * 4, embedding_dim),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "jQcyBsNsaNYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, embedding_dim, head_dim, num_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(embedding_dim, head_dim, num_heads)\n",
        "    self.ffc = FeedForward(embedding_dim)\n",
        "    self.ln1 = nn.LayerNorm(embedding_dim)\n",
        "    self.ln2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.ln1(self.mha(x))\n",
        "    x = x + self.ln2(self.ffc(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "ITwUoDq_alR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, head_dim, num_heads, num_layers, block_size):\n",
        "    super().__init__()\n",
        "\n",
        "    # initalize word embedding and positional embedding\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.pos_embedding = nn.Embedding(block_size, embedding_dim)\n",
        "\n",
        "    # transformer works here\n",
        "    self.blocks = nn.Sequential(*[Block(embedding_dim, head_dim, num_heads) for _ in range(num_layers)])\n",
        "\n",
        "    # normalization layer\n",
        "    self.ln = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    # classification layer\n",
        "    self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x, target=None):\n",
        "    B, T = x.shape\n",
        "\n",
        "    # get embeddings\n",
        "    embedding = self.embedding(x)\n",
        "    pos_embedding = self.pos_embedding(torch.arange(T, device=x.device))\n",
        "\n",
        "    inputs = embedding + pos_embedding\n",
        "\n",
        "    out = self.blocks(inputs)\n",
        "    out = self.ln(out)\n",
        "    logits = self.linear(out)\n",
        "\n",
        "    if target is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      pass\n",
        "      # logits = logits.view(B * T, C)\n",
        "      # target = target.view(B, T)\n",
        "      # loss = F.cross_entropy(logits, target)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "ELyXp4kzbOJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparing a tiny-Dataset**"
      ],
      "metadata": {
        "id": "rQubtAyTfFTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read our abay.txt\n",
        "with open(\"abay.txt\", 'r') as file:\n",
        "  data = file.read()\n",
        "\n",
        "data = data.split('\\n')\n",
        "data = \" \".join(data)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "9m8_excMmSIw",
        "outputId": "4e01f1de-4a6e-42e1-b427-f16853c5abe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Өлең - сөздің патшасы, сөз сарасы, Қиыннан қиыстырар ер данасы. Тілге жеңіл, жүрекке жылы тиіп, Теп-тегіс жұмыр келсін айналасы. Бөтен сөзбен былғанса сөз арасы, Ол - ақынның білімсіз бишарасы. Айтушы мен тыңдаушы көбі надан, Бұл жұрттың сөз танымас бір парасы. Әуелі хаят, хәдис - сөздің басы, Қосарлы бәйітмысал келді арасы. Қисынымен қызықты болмаса сөз, Неге айтсын пайғамбар мен оны алласы. Мешіттің құтпа оқыған ғұламасы, Мүнәжәт уәлилердің зар наласы. Бір сөзін бір сөзіне қиыстырар, Әрбірі келгенінше өз шамасы. Өлеңге әркімнің-ақ бар таласы, Сонда да солардың бар таңдамасы. Іші алтын, сырты күміс сөз жақсысын Қазақтың келістірер қай баласы? Бұрынғы ескі биді тұрсам барлап, Мақалдап айтады екен, сөз қосарлап. Ақындары ақылсыз, надан келіп, Көр-жерді өлең қыпты жоқтан қармап. Қобыз бен домбыра алып топта сарнап, Мақтау өлең айтыпты әркімге арнап. Әр елден өлеңменен қайыр тілеп, Кетірген сөз қадірін жұртты шарлап. Мал үшін тілін безеп, жанып жалдап, Мал сұрап біреуді алдап, біреуді арбап. Жат елде қайыршылық қылып жүріп, Өз елін бай деп мақтар құдай қарғап. Қайда бай мақтаншаққа барған таңдап, Жиса да, бай болмапты, қанша малды ап. Казаққа өлең деген бір қадірсіз, Былжырақ көрінеді солар даңдақ. Ескі бише отырман бос мақалдап, Ескі ақынша мал үшін тұрман зарлап. Сөз түзелді, тыңдаушы, сен де түзел, Сендерге де келейін енді аяңдап. Батырды айтсам ел шауып алған талап, Қызды айтсам, кызықты айтсам қыздырмалап, Әншейін күн өткізбек әңгімеге Тыңдар едің әр сөзін мыңға балап. Ақыл сөзге ынтасыз, жұрт шабандап, Көнгенім-ақ соған деп жүр табандап. Кісімсінген жеп кетер білімсіз көп, Жіберсем, өкпелеме, көп жамандап. Амалдап қарағайды талға жалғап, Әркім жүр алар жердің ебін қамдап. Мақтан қуған, малқұмар нені ұға алсын, Шықпаса мыңнан біреу талғап-талғап. Мал жиып арамдықпен ұрлап-қарлап, Қусың десе, қуанып жүр алшаңдап. Қақса-соқса бір пайда түсе ме деп, Елдің байын еліртіп «жау мұндалап». Ынсап, ұят, ар, намыс, сабыр, талап - Бұларды керек қылмас ешкім қалап. Терең ой, терең ғылым іздемейді, Өтірік пен өсекті жүндей сабап.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text clean function\n",
        "def clean_text(text):\n",
        "  # convert to lower case\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove some unnecessary characters\n",
        "  cleaned_text = re.sub(r\"[^a-zа-яәіңғүұқөһ\\s-]\", \"\", text)\n",
        "  cleaned_text = re.sub(r\"\\\\s+\", \" \", cleaned_text)\n",
        "\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "20jX4GKlml0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = clean_text(data)\n",
        "cleaned_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "TG4dA9iVmw5x",
        "outputId": "868f6756-b0ce-4a44-fd55-e1ca6e64c20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'өлең - сөздің патшасы сөз сарасы қиыннан қиыстырар ер данасы тілге жеңіл жүрекке жылы тиіп теп-тегіс жұмыр келсін айналасы бөтен сөзбен былғанса сөз арасы ол - ақынның білімсіз бишарасы айтушы мен тыңдаушы көбі надан бұл жұрттың сөз танымас бір парасы әуелі хаят хәдис - сөздің басы қосарлы бәйітмысал келді арасы қисынымен қызықты болмаса сөз неге айтсын пайғамбар мен оны алласы мешіттің құтпа оқыған ғұламасы мүнәжәт уәлилердің зар наласы бір сөзін бір сөзіне қиыстырар әрбірі келгенінше өз шамасы өлеңге әркімнің-ақ бар таласы сонда да солардың бар таңдамасы іші алтын сырты күміс сөз жақсысын қазақтың келістірер қай баласы бұрынғы ескі биді тұрсам барлап мақалдап айтады екен сөз қосарлап ақындары ақылсыз надан келіп көр-жерді өлең қыпты жоқтан қармап қобыз бен домбыра алып топта сарнап мақтау өлең айтыпты әркімге арнап әр елден өлеңменен қайыр тілеп кетірген сөз қадірін жұртты шарлап мал үшін тілін безеп жанып жалдап мал сұрап біреуді алдап біреуді арбап жат елде қайыршылық қылып жүріп өз елін бай деп мақтар құдай қарғап қайда бай мақтаншаққа барған таңдап жиса да бай болмапты қанша малды ап казаққа өлең деген бір қадірсіз былжырақ көрінеді солар даңдақ ескі бише отырман бос мақалдап ескі ақынша мал үшін тұрман зарлап сөз түзелді тыңдаушы сен де түзел сендерге де келейін енді аяңдап батырды айтсам ел шауып алған талап қызды айтсам кызықты айтсам қыздырмалап әншейін күн өткізбек әңгімеге тыңдар едің әр сөзін мыңға балап ақыл сөзге ынтасыз жұрт шабандап көнгенім-ақ соған деп жүр табандап кісімсінген жеп кетер білімсіз көп жіберсем өкпелеме көп жамандап амалдап қарағайды талға жалғап әркім жүр алар жердің ебін қамдап мақтан қуған малқұмар нені ұға алсын шықпаса мыңнан біреу талғап-талғап мал жиып арамдықпен ұрлап-қарлап қусың десе қуанып жүр алшаңдап қақса-соқса бір пайда түсе ме деп елдің байын еліртіп жау мұндалап ынсап ұят ар намыс сабыр талап - бұларды керек қылмас ешкім қалап терең ой терең ғылым іздемейді өтірік пен өсекті жүндей сабап'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"How words in our dataset: {len(cleaned_text.split())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhpGz_qym_MW",
        "outputId": "fcf20317-ba91-44f5-ebd5-4b851d93df46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How words in our dataset: 305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "iGQ2OMPfnIG7",
        "outputId": "19c6a594-7730-4729-8dbd-14cbfbb5ef73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'өлең - сөздің патшасы сөз сарасы қиыннан қиыстырар ер данасы тілге жеңіл жүрекке жылы тиіп теп-тегіс жұмыр келсін айналасы бөтен сөзбен былғанса сөз арасы ол - ақынның білімсіз бишарасы айтушы мен тыңдаушы көбі надан бұл жұрттың сөз танымас бір парасы әуелі хаят хәдис - сөздің басы қосарлы бәйітмысал келді арасы қисынымен қызықты болмаса сөз неге айтсын пайғамбар мен оны алласы мешіттің құтпа оқыған ғұламасы мүнәжәт уәлилердің зар наласы бір сөзін бір сөзіне қиыстырар әрбірі келгенінше өз шамасы өлеңге әркімнің-ақ бар таласы сонда да солардың бар таңдамасы іші алтын сырты күміс сөз жақсысын қазақтың келістірер қай баласы бұрынғы ескі биді тұрсам барлап мақалдап айтады екен сөз қосарлап ақындары ақылсыз надан келіп көр-жерді өлең қыпты жоқтан қармап қобыз бен домбыра алып топта сарнап мақтау өлең айтыпты әркімге арнап әр елден өлеңменен қайыр тілеп кетірген сөз қадірін жұртты шарлап мал үшін тілін безеп жанып жалдап мал сұрап біреуді алдап біреуді арбап жат елде қайыршылық қылып жүріп өз елін бай деп мақтар құдай қарғап қайда бай мақтаншаққа барған таңдап жиса да бай болмапты қанша малды ап казаққа өлең деген бір қадірсіз былжырақ көрінеді солар даңдақ ескі бише отырман бос мақалдап ескі ақынша мал үшін тұрман зарлап сөз түзелді тыңдаушы сен де түзел сендерге де келейін енді аяңдап батырды айтсам ел шауып алған талап қызды айтсам кызықты айтсам қыздырмалап әншейін күн өткізбек әңгімеге тыңдар едің әр сөзін мыңға балап ақыл сөзге ынтасыз жұрт шабандап көнгенім-ақ соған деп жүр табандап кісімсінген жеп кетер білімсіз көп жіберсем өкпелеме көп жамандап амалдап қарағайды талға жалғап әркім жүр алар жердің ебін қамдап мақтан қуған малқұмар нені ұға алсын шықпаса мыңнан біреу талғап-талғап мал жиып арамдықпен ұрлап-қарлап қусың десе қуанып жүр алшаңдап қақса-соқса бір пайда түсе ме деп елдің байын еліртіп жау мұндалап ынсап ұят ар намыс сабыр талап - бұларды керек қылмас ешкім қалап терең ой терең ғылым іздемейді өтірік пен өсекті жүндей сабап'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparing a tokenizer**"
      ],
      "metadata": {
        "id": "fZauSPTkmN9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class tokenizer():\n",
        "  def __init__(self):\n",
        "    self.char2idx = {\"<PAD>\":0, \"<BOS>\": 1, \"EOS\": 2, \"<UNK>\": 3}\n",
        "    self.idx2char = {0: \"<PAD>\", 1: \"<BOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "    self.count_char = 4\n",
        "\n",
        "  def train(self, corpus):\n",
        "    sorted_chars = list(sorted(Counter(corpus)))\n",
        "\n",
        "    for index, char in enumerate(sorted_chars, start=4):\n",
        "      self.char2idx[char] = index\n",
        "      self.idx2char[index] = char\n",
        "      self.count_char += 1\n",
        "  def __len__(self):\n",
        "    return self.count_char"
      ],
      "metadata": {
        "id": "-1pWKw_llfp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tokenizer()"
      ],
      "metadata": {
        "id": "mimsOuWan9xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(cleaned_text)"
      ],
      "metadata": {
        "id": "0KhgDRTAn_UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparing a training dataset**"
      ],
      "metadata": {
        "id": "_2e6y0_Po2nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = [tokenizer.char2idx[char] for char in cleaned_text]\n",
        "tokenized_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ9M-2Eeo8dI",
        "outputId": "4fd14326-a3fd-421c-e5a1-cc0a6380abba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[36,\n",
              " 16,\n",
              " 10,\n",
              " 32,\n",
              " 4,\n",
              " 5,\n",
              " 4,\n",
              " 22,\n",
              " 36,\n",
              " 12,\n",
              " 9,\n",
              " 29,\n",
              " 32,\n",
              " 4,\n",
              " 20,\n",
              " 6,\n",
              " 23,\n",
              " 26,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 22,\n",
              " 36,\n",
              " 12,\n",
              " 4,\n",
              " 22,\n",
              " 6,\n",
              " 21,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 31,\n",
              " 13,\n",
              " 27,\n",
              " 18,\n",
              " 18,\n",
              " 6,\n",
              " 18,\n",
              " 4,\n",
              " 31,\n",
              " 13,\n",
              " 27,\n",
              " 22,\n",
              " 23,\n",
              " 27,\n",
              " 21,\n",
              " 6,\n",
              " 21,\n",
              " 4,\n",
              " 10,\n",
              " 21,\n",
              " 4,\n",
              " 9,\n",
              " 6,\n",
              " 18,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 23,\n",
              " 29,\n",
              " 16,\n",
              " 8,\n",
              " 10,\n",
              " 4,\n",
              " 11,\n",
              " 10,\n",
              " 32,\n",
              " 29,\n",
              " 16,\n",
              " 4,\n",
              " 11,\n",
              " 33,\n",
              " 21,\n",
              " 10,\n",
              " 15,\n",
              " 15,\n",
              " 10,\n",
              " 4,\n",
              " 11,\n",
              " 27,\n",
              " 16,\n",
              " 27,\n",
              " 4,\n",
              " 23,\n",
              " 13,\n",
              " 29,\n",
              " 20,\n",
              " 4,\n",
              " 23,\n",
              " 10,\n",
              " 20,\n",
              " 5,\n",
              " 23,\n",
              " 10,\n",
              " 8,\n",
              " 29,\n",
              " 22,\n",
              " 4,\n",
              " 11,\n",
              " 34,\n",
              " 17,\n",
              " 27,\n",
              " 21,\n",
              " 4,\n",
              " 15,\n",
              " 10,\n",
              " 16,\n",
              " 22,\n",
              " 29,\n",
              " 18,\n",
              " 4,\n",
              " 6,\n",
              " 14,\n",
              " 18,\n",
              " 6,\n",
              " 16,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 7,\n",
              " 36,\n",
              " 23,\n",
              " 10,\n",
              " 18,\n",
              " 4,\n",
              " 22,\n",
              " 36,\n",
              " 12,\n",
              " 7,\n",
              " 10,\n",
              " 18,\n",
              " 4,\n",
              " 7,\n",
              " 27,\n",
              " 16,\n",
              " 30,\n",
              " 6,\n",
              " 18,\n",
              " 22,\n",
              " 6,\n",
              " 4,\n",
              " 22,\n",
              " 36,\n",
              " 12,\n",
              " 4,\n",
              " 6,\n",
              " 21,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 19,\n",
              " 16,\n",
              " 4,\n",
              " 5,\n",
              " 4,\n",
              " 6,\n",
              " 31,\n",
              " 27,\n",
              " 18,\n",
              " 18,\n",
              " 27,\n",
              " 32,\n",
              " 4,\n",
              " 7,\n",
              " 29,\n",
              " 16,\n",
              " 29,\n",
              " 17,\n",
              " 22,\n",
              " 29,\n",
              " 12,\n",
              " 4,\n",
              " 7,\n",
              " 13,\n",
              " 26,\n",
              " 6,\n",
              " 21,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 6,\n",
              " 14,\n",
              " 23,\n",
              " 24,\n",
              " 26,\n",
              " 27,\n",
              " 4,\n",
              " 17,\n",
              " 10,\n",
              " 18,\n",
              " 4,\n",
              " 23,\n",
              " 27,\n",
              " 32,\n",
              " 9,\n",
              " 6,\n",
              " 24,\n",
              " 26,\n",
              " 27,\n",
              " 4,\n",
              " 15,\n",
              " 36,\n",
              " 7,\n",
              " 29,\n",
              " 4,\n",
              " 18,\n",
              " 6,\n",
              " 9,\n",
              " 6,\n",
              " 18,\n",
              " 4,\n",
              " 7,\n",
              " 34,\n",
              " 16,\n",
              " 4,\n",
              " 11,\n",
              " 34,\n",
              " 21,\n",
              " 23,\n",
              " 23,\n",
              " 27,\n",
              " 32,\n",
              " 4,\n",
              " 22,\n",
              " 36,\n",
              " 12,\n",
              " 4,\n",
              " 23,\n",
              " 6,\n",
              " 18,\n",
              " 27,\n",
              " 17,\n",
              " 6,\n",
              " 22,\n",
              " 4,\n",
              " 7,\n",
              " 29,\n",
              " 21,\n",
              " 4,\n",
              " 20,\n",
              " 6,\n",
              " 21,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 35,\n",
              " 24,\n",
              " 10,\n",
              " 16,\n",
              " 29,\n",
              " 4,\n",
              " 25,\n",
              " 6,\n",
              " 28,\n",
              " 23,\n",
              " 4,\n",
              " 25,\n",
              " 35,\n",
              " 9,\n",
              " 13,\n",
              " 22,\n",
              " 4,\n",
              " 5,\n",
              " 4,\n",
              " 22,\n",
              " 36,\n",
              " 12,\n",
              " 9,\n",
              " 29,\n",
              " 32,\n",
              " 4,\n",
              " 7,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 31,\n",
              " 19,\n",
              " 22,\n",
              " 6,\n",
              " 21,\n",
              " 16,\n",
              " 27,\n",
              " 4,\n",
              " 7,\n",
              " 35,\n",
              " 14,\n",
              " 29,\n",
              " 23,\n",
              " 17,\n",
              " 27,\n",
              " 22,\n",
              " 6,\n",
              " 16,\n",
              " 4,\n",
              " 15,\n",
              " 10,\n",
              " 16,\n",
              " 9,\n",
              " 29,\n",
              " 4,\n",
              " 6,\n",
              " 21,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 31,\n",
              " 13,\n",
              " 22,\n",
              " 27,\n",
              " 18,\n",
              " 27,\n",
              " 17,\n",
              " 10,\n",
              " 18,\n",
              " 4,\n",
              " 31,\n",
              " 27,\n",
              " 12,\n",
              " 27,\n",
              " 31,\n",
              " 23,\n",
              " 27,\n",
              " 4,\n",
              " 7,\n",
              " 19,\n",
              " 16,\n",
              " 17,\n",
              " 6,\n",
              " 22,\n",
              " 6,\n",
              " 4,\n",
              " 22,\n",
              " 36,\n",
              " 12,\n",
              " 4,\n",
              " 18,\n",
              " 10,\n",
              " 8,\n",
              " 10,\n",
              " 4,\n",
              " 6,\n",
              " 14,\n",
              " 23,\n",
              " 22,\n",
              " 27,\n",
              " 18,\n",
              " 4,\n",
              " 20,\n",
              " 6,\n",
              " 14,\n",
              " 30,\n",
              " 6,\n",
              " 17,\n",
              " 7,\n",
              " 6,\n",
              " 21,\n",
              " 4,\n",
              " 17,\n",
              " 10,\n",
              " 18,\n",
              " 4,\n",
              " 19,\n",
              " 18,\n",
              " 27,\n",
              " 4,\n",
              " 6,\n",
              " 16,\n",
              " 16,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 17,\n",
              " 10,\n",
              " 26,\n",
              " 29,\n",
              " 23,\n",
              " 23,\n",
              " 29,\n",
              " 32,\n",
              " 4,\n",
              " 31,\n",
              " 34,\n",
              " 23,\n",
              " 20,\n",
              " 6,\n",
              " 4,\n",
              " 19,\n",
              " 31,\n",
              " 27,\n",
              " 30,\n",
              " 6,\n",
              " 18,\n",
              " 4,\n",
              " 30,\n",
              " 34,\n",
              " 16,\n",
              " 6,\n",
              " 17,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 17,\n",
              " 33,\n",
              " 18,\n",
              " 35,\n",
              " 11,\n",
              " 35,\n",
              " 23,\n",
              " 4,\n",
              " 24,\n",
              " 35,\n",
              " 16,\n",
              " 13,\n",
              " 16,\n",
              " 10,\n",
              " 21,\n",
              " 9,\n",
              " 29,\n",
              " 32,\n",
              " 4,\n",
              " 12,\n",
              " 6,\n",
              " 21,\n",
              " 4,\n",
              " 18,\n",
              " 6,\n",
              " 16,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 7,\n",
              " 29,\n",
              " 21,\n",
              " 4,\n",
              " 22,\n",
              " 36,\n",
              " 12,\n",
              " 29,\n",
              " 18,\n",
              " 4,\n",
              " 7,\n",
              " 29,\n",
              " 21,\n",
              " 4,\n",
              " 22,\n",
              " 36,\n",
              " 12,\n",
              " 29,\n",
              " 18,\n",
              " 10,\n",
              " 4,\n",
              " 31,\n",
              " 13,\n",
              " 27,\n",
              " 22,\n",
              " 23,\n",
              " 27,\n",
              " 21,\n",
              " 6,\n",
              " 21,\n",
              " 4,\n",
              " 35,\n",
              " 21,\n",
              " 7,\n",
              " 29,\n",
              " 21,\n",
              " 29,\n",
              " 4,\n",
              " 15,\n",
              " 10,\n",
              " 16,\n",
              " 8,\n",
              " 10,\n",
              " 18,\n",
              " 29,\n",
              " 18,\n",
              " 26,\n",
              " 10,\n",
              " 4,\n",
              " 36,\n",
              " 12,\n",
              " 4,\n",
              " 26,\n",
              " 6,\n",
              " 17,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 36,\n",
              " 16,\n",
              " 10,\n",
              " 32,\n",
              " 8,\n",
              " 10,\n",
              " 4,\n",
              " 35,\n",
              " 21,\n",
              " 15,\n",
              " 29,\n",
              " 17,\n",
              " 18,\n",
              " 29,\n",
              " 32,\n",
              " 5,\n",
              " 6,\n",
              " 31,\n",
              " 4,\n",
              " 7,\n",
              " 6,\n",
              " 21,\n",
              " 4,\n",
              " 23,\n",
              " 6,\n",
              " 16,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 22,\n",
              " 19,\n",
              " 18,\n",
              " 9,\n",
              " 6,\n",
              " 4,\n",
              " 9,\n",
              " 6,\n",
              " 4,\n",
              " 22,\n",
              " 19,\n",
              " 16,\n",
              " 6,\n",
              " 21,\n",
              " 9,\n",
              " 27,\n",
              " 32,\n",
              " 4,\n",
              " 7,\n",
              " 6,\n",
              " 21,\n",
              " 4,\n",
              " 23,\n",
              " 6,\n",
              " 32,\n",
              " 9,\n",
              " 6,\n",
              " 17,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 29,\n",
              " 26,\n",
              " 29,\n",
              " 4,\n",
              " 6,\n",
              " 16,\n",
              " 23,\n",
              " 27,\n",
              " 18,\n",
              " 4,\n",
              " 22,\n",
              " 27,\n",
              " 21,\n",
              " 23,\n",
              " 27,\n",
              " 4,\n",
              " 15,\n",
              " 33,\n",
              " 17,\n",
              " 29,\n",
              " 22,\n",
              " 4,\n",
              " 22,\n",
              " 36,\n",
              " 12,\n",
              " 4,\n",
              " 11,\n",
              " 6,\n",
              " 31,\n",
              " 22,\n",
              " 27,\n",
              " 22,\n",
              " 27,\n",
              " 18,\n",
              " 4,\n",
              " 31,\n",
              " 6,\n",
              " 12,\n",
              " 6,\n",
              " 31,\n",
              " 23,\n",
              " 27,\n",
              " 32,\n",
              " 4,\n",
              " 15,\n",
              " 10,\n",
              " 16,\n",
              " 29,\n",
              " 22,\n",
              " 23,\n",
              " 29,\n",
              " 21,\n",
              " 10,\n",
              " 21,\n",
              " 4,\n",
              " 31,\n",
              " 6,\n",
              " 14,\n",
              " 4,\n",
              " 7,\n",
              " 6,\n",
              " 16,\n",
              " 6,\n",
              " 22,\n",
              " 27,\n",
              " 4,\n",
              " 7,\n",
              " 34,\n",
              " 21,\n",
              " 27,\n",
              " 18,\n",
              " 30,\n",
              " 27,\n",
              " 4,\n",
              " 10,\n",
              " 22,\n",
              " 15,\n",
              " 29,\n",
              " 4,\n",
              " 7,\n",
              " 13,\n",
              " 9,\n",
              " 29,\n",
              " 4,\n",
              " 23,\n",
              " 34,\n",
              " 21,\n",
              " 22,\n",
              " 6,\n",
              " 17,\n",
              " 4,\n",
              " 7,\n",
              " 6,\n",
              " 21,\n",
              " 16,\n",
              " 6,\n",
              " 20,\n",
              " 4,\n",
              " 17,\n",
              " 6,\n",
              " 31,\n",
              " 6,\n",
              " 16,\n",
              " 9,\n",
              " 6,\n",
              " 20,\n",
              " 4,\n",
              " 6,\n",
              " 14,\n",
              " 23,\n",
              " 6,\n",
              " 9,\n",
              " 27,\n",
              " 4,\n",
              " 10,\n",
              " 15,\n",
              " 10,\n",
              " 18,\n",
              " 4,\n",
              " 22,\n",
              " 36,\n",
              " 12,\n",
              " 4,\n",
              " 31,\n",
              " 19,\n",
              " 22,\n",
              " 6,\n",
              " 21,\n",
              " 16,\n",
              " 6,\n",
              " 20,\n",
              " 4,\n",
              " 6,\n",
              " 31,\n",
              " 27,\n",
              " 18,\n",
              " 9,\n",
              " 6,\n",
              " 21,\n",
              " 27,\n",
              " 4,\n",
              " 6,\n",
              " 31,\n",
              " 27,\n",
              " 16,\n",
              " 22,\n",
              " 27,\n",
              " 12,\n",
              " 4,\n",
              " 18,\n",
              " 6,\n",
              " 9,\n",
              " 6,\n",
              " 18,\n",
              " 4,\n",
              " 15,\n",
              " 10,\n",
              " 16,\n",
              " 29,\n",
              " 20,\n",
              " 4,\n",
              " 15,\n",
              " 36,\n",
              " 21,\n",
              " 5,\n",
              " 11,\n",
              " 10,\n",
              " 21,\n",
              " 9,\n",
              " 29,\n",
              " 4,\n",
              " 36,\n",
              " 16,\n",
              " 10,\n",
              " 32,\n",
              " 4,\n",
              " 31,\n",
              " 27,\n",
              " 20,\n",
              " 23,\n",
              " 27,\n",
              " 4,\n",
              " 11,\n",
              " 19,\n",
              " 31,\n",
              " 23,\n",
              " 6,\n",
              " 18,\n",
              " 4,\n",
              " 31,\n",
              " 6,\n",
              " 21,\n",
              " 17,\n",
              " 6,\n",
              " 20,\n",
              " 4,\n",
              " 31,\n",
              " 19,\n",
              " 7,\n",
              " 27,\n",
              " 12,\n",
              " 4,\n",
              " 7,\n",
              " 10,\n",
              " 18,\n",
              " 4,\n",
              " 9,\n",
              " 19,\n",
              " 17,\n",
              " 7,\n",
              " 27,\n",
              " 21,\n",
              " 6,\n",
              " 4,\n",
              " 6,\n",
              " 16,\n",
              " 27,\n",
              " 20,\n",
              " 4,\n",
              " 23,\n",
              " 19,\n",
              " 20,\n",
              " 23,\n",
              " 6,\n",
              " 4,\n",
              " 22,\n",
              " 6,\n",
              " 21,\n",
              " 18,\n",
              " 6,\n",
              " 20,\n",
              " 4,\n",
              " 17,\n",
              " 6,\n",
              " 31,\n",
              " 23,\n",
              " 6,\n",
              " 24,\n",
              " 4,\n",
              " 36,\n",
              " 16,\n",
              " 10,\n",
              " 32,\n",
              " 4,\n",
              " 6,\n",
              " 14,\n",
              " 23,\n",
              " 27,\n",
              " 20,\n",
              " 23,\n",
              " 27,\n",
              " 4,\n",
              " 35,\n",
              " 21,\n",
              " 15,\n",
              " 29,\n",
              " 17,\n",
              " 8,\n",
              " 10,\n",
              " 4,\n",
              " 6,\n",
              " 21,\n",
              " 18,\n",
              " 6,\n",
              " 20,\n",
              " 4,\n",
              " 35,\n",
              " 21,\n",
              " 4,\n",
              " 10,\n",
              " 16,\n",
              " 9,\n",
              " 10,\n",
              " 18,\n",
              " 4,\n",
              " 36,\n",
              " 16,\n",
              " 10,\n",
              " 32,\n",
              " 17,\n",
              " 10,\n",
              " 18,\n",
              " 10,\n",
              " 18,\n",
              " 4,\n",
              " 31,\n",
              " 6,\n",
              " 14,\n",
              " 27,\n",
              " 21,\n",
              " 4,\n",
              " 23,\n",
              " 29,\n",
              " 16,\n",
              " 10,\n",
              " 20,\n",
              " 4,\n",
              " 15,\n",
              " 10,\n",
              " 23,\n",
              " 29,\n",
              " 21,\n",
              " 8,\n",
              " 10,\n",
              " 18,\n",
              " 4,\n",
              " 22,\n",
              " 36,\n",
              " 12,\n",
              " 4,\n",
              " 31,\n",
              " 6,\n",
              " 9,\n",
              " 29,\n",
              " 21,\n",
              " 29,\n",
              " 18,\n",
              " 4,\n",
              " 11,\n",
              " 34,\n",
              " 21,\n",
              " 23,\n",
              " 23,\n",
              " 27,\n",
              " 4,\n",
              " 26,\n",
              " 6,\n",
              " 21,\n",
              " 16,\n",
              " 6,\n",
              " 20,\n",
              " 4,\n",
              " 17,\n",
              " 6,\n",
              " 16,\n",
              " 4,\n",
              " 33,\n",
              " 26,\n",
              " 29,\n",
              " 18,\n",
              " 4,\n",
              " 23,\n",
              " 29,\n",
              " 16,\n",
              " 29,\n",
              " 18,\n",
              " 4,\n",
              " 7,\n",
              " 10,\n",
              " 12,\n",
              " 10,\n",
              " 20,\n",
              " 4,\n",
              " 11,\n",
              " 6,\n",
              " 18,\n",
              " 27,\n",
              " 20,\n",
              " 4,\n",
              " 11,\n",
              " 6,\n",
              " 16,\n",
              " 9,\n",
              " 6,\n",
              " 20,\n",
              " 4,\n",
              " 17,\n",
              " 6,\n",
              " 16,\n",
              " 4,\n",
              " 22,\n",
              " 34,\n",
              " 21,\n",
              " 6,\n",
              " 20,\n",
              " 4,\n",
              " 7,\n",
              " 29,\n",
              " 21,\n",
              " 10,\n",
              " 24,\n",
              " 9,\n",
              " 29,\n",
              " 4,\n",
              " 6,\n",
              " 16,\n",
              " 9,\n",
              " 6,\n",
              " 20,\n",
              " 4,\n",
              " 7,\n",
              " 29,\n",
              " 21,\n",
              " 10,\n",
              " 24,\n",
              " 9,\n",
              " 29,\n",
              " 4,\n",
              " 6,\n",
              " 21,\n",
              " 7,\n",
              " 6,\n",
              " 20,\n",
              " 4,\n",
              " 11,\n",
              " 6,\n",
              " 23,\n",
              " 4,\n",
              " 10,\n",
              " 16,\n",
              " 9,\n",
              " 10,\n",
              " 4,\n",
              " 31,\n",
              " 6,\n",
              " 14,\n",
              " 27,\n",
              " 21,\n",
              " 26,\n",
              " 27,\n",
              " 16,\n",
              " 27,\n",
              " 31,\n",
              " 4,\n",
              " 31,\n",
              " 27,\n",
              " 16,\n",
              " 27,\n",
              " 20,\n",
              " 4,\n",
              " 11,\n",
              " 33,\n",
              " 21,\n",
              " 29,\n",
              " 20,\n",
              " 4,\n",
              " 36,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_sequences = []\n",
        "y_sequences = []"
      ],
      "metadata": {
        "id": "q_1z_ar3pMft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a X and y\n",
        "\n",
        "for idx in range(len(tokenized_text) - 128):\n",
        "  X_sequences.append(tokenized_text[idx : idx + 128])\n",
        "  y_sequences.append(tokenized_text[idx + 1: idx + 129])"
      ],
      "metadata": {
        "id": "Npk-6XJSpFRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTdataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = self.X[idx]\n",
        "    y = self.y[idx]\n",
        "\n",
        "    x = torch.tensor(x, dtype=torch.long)\n",
        "    y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "fV9f1PdZqjOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = GPTdataset(X_sequences, y_sequences)"
      ],
      "metadata": {
        "id": "COZYGap5q3q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "H9zwx8I_q7Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "  inputs, outputs = batch\n",
        "  print(f\"input.shape: {inputs.shape}, output.shape: {outputs.shape}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxHbT3ufrKND",
        "outputId": "515c7737-c71d-4802-d1db-03db5d26eabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input.shape: torch.Size([16, 128]), output.shape: torch.Size([16, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialize a model**"
      ],
      "metadata": {
        "id": "p6_Xjv00phRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONFIGURATION**"
      ],
      "metadata": {
        "id": "g26WvNuYplpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "EMBEDDING_DIM = 64\n",
        "NUM_HEADS = 4\n",
        "HEAD_DIM = 16\n",
        "BLOCK_SIZE = 128\n",
        "NUM_LAYERS = 4\n",
        "VOCAB_SIZE = tokenizer.count_char"
      ],
      "metadata": {
        "id": "KuTjHH1DponI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTmodel(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, head_dim=HEAD_DIM, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, block_size=BLOCK_SIZE)"
      ],
      "metadata": {
        "id": "aRo-PmIJp1T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "dJmIA7qyrk-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**optimizer and loss function**"
      ],
      "metadata": {
        "id": "EWOuPeRHqKzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Yw0G7TxvqOMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train a model**"
      ],
      "metadata": {
        "id": "3GOLlAmhqVjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model.train()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch in train_loader:\n",
        "    inputs, labels = batch\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    B, T, C = outputs.shape\n",
        "\n",
        "    outputs = outputs.view(B * T, C)\n",
        "    labels = labels.view(B * T)\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f\"epoch: {epoch}, loss: {total_loss / len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkwRLCqwqXUk",
        "outputId": "b09b4e0a-7613-465f-ceb6-1d39d3d171f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, loss: 3.068751349531371\n",
            "epoch: 2, loss: 2.631049920772684\n",
            "epoch: 3, loss: 2.4552911622770903\n",
            "epoch: 4, loss: 2.356138889131875\n",
            "epoch: 5, loss: 2.2906295225538056\n",
            "epoch: 6, loss: 2.2363770028640486\n",
            "epoch: 7, loss: 2.1908835069886567\n",
            "epoch: 8, loss: 2.144235397207326\n",
            "epoch: 9, loss: 2.098283255922383\n",
            "epoch: 10, loss: 2.0492276878192506\n",
            "epoch: 11, loss: 1.997600286171354\n",
            "epoch: 12, loss: 1.9450071741794717\n",
            "epoch: 13, loss: 1.8920302576032177\n",
            "epoch: 14, loss: 1.836327310266166\n",
            "epoch: 15, loss: 1.7794102389236977\n",
            "epoch: 16, loss: 1.726103829926458\n",
            "epoch: 17, loss: 1.6689497242713798\n",
            "epoch: 18, loss: 1.6138427339751145\n",
            "epoch: 19, loss: 1.5585867135689175\n",
            "epoch: 20, loss: 1.5061595665997471\n",
            "epoch: 21, loss: 1.4472676197002674\n",
            "epoch: 22, loss: 1.3950767034086689\n",
            "epoch: 23, loss: 1.3419896395042026\n",
            "epoch: 24, loss: 1.2896447263914963\n",
            "epoch: 25, loss: 1.2378859653555114\n",
            "epoch: 26, loss: 1.192848765644534\n",
            "epoch: 27, loss: 1.1442728093985854\n",
            "epoch: 28, loss: 1.094832689597689\n",
            "epoch: 29, loss: 1.0509170766534477\n",
            "epoch: 30, loss: 1.0065256639801223\n",
            "epoch: 31, loss: 0.9646946279139355\n",
            "epoch: 32, loss: 0.9254888789407139\n",
            "epoch: 33, loss: 0.8829085955332066\n",
            "epoch: 34, loss: 0.8447262742396059\n",
            "epoch: 35, loss: 0.8066013537604233\n",
            "epoch: 36, loss: 0.7697397347154289\n",
            "epoch: 37, loss: 0.7388936815590694\n",
            "epoch: 38, loss: 0.7074700635055016\n",
            "epoch: 39, loss: 0.6757388001885908\n",
            "epoch: 40, loss: 0.646540951112221\n",
            "epoch: 41, loss: 0.6176225635512121\n",
            "epoch: 42, loss: 0.5910243345745678\n",
            "epoch: 43, loss: 0.5657107344475286\n",
            "epoch: 44, loss: 0.5418269929700884\n",
            "epoch: 45, loss: 0.5213216476399323\n",
            "epoch: 46, loss: 0.4989038844560755\n",
            "epoch: 47, loss: 0.479844532393176\n",
            "epoch: 48, loss: 0.46004077946317606\n",
            "epoch: 49, loss: 0.4402989465101012\n",
            "epoch: 50, loss: 0.42289963261834507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference**"
      ],
      "metadata": {
        "id": "ttyYT7chJQqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_(model, start_text, max_new_tokens=100, temperature=1.0):\n",
        "  model.eval()\n",
        "\n",
        "  idx = [tokenizer.char2idx.get(char, tokenizer.char2idx[\"<UNK>\"]) for char in start_text.lower()]\n",
        "\n",
        "  idx = torch.tensor(idx, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx = idx if idx.size(1) <= BLOCK_SIZE else idx[:, -BLOCK_SIZE:]\n",
        "\n",
        "      logits = model(idx)\n",
        "\n",
        "      logits = logits[:, -1, :]\n",
        "\n",
        "      logits = logits / temperature\n",
        "      logits = F.softmax(logits, dim=-1)\n",
        "\n",
        "      idx_next = torch.multinomial(logits, num_samples=1)\n",
        "\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "  result_idx = idx[0].tolist()\n",
        "  result = \"\".join(tokenizer.idx2char[i] for i in result_idx)\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "2Td08PYrJSlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_(model, start_text=\"өлең\", max_new_tokens=200, temperature=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "l47tDoaPMVbr",
        "outputId": "c3b966cc-ede1-4548-bb11-2f6705d69034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ірезезеп жаліреуді жап алдап біреуді алдап барбатап е қайылыршық қылып жүріп жүріп еліп бай марін деп бақтай құдарға қайда бай ма'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_(model, start_text=\"мен\", max_new_tokens=200, temperature=0.5)"
      ],
      "metadata": {
        "id": "nO4rXtc44hui",
        "outputId": "30d2418d-5785-4d25-8263-9354a29381b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ысал келді асы арасы қисыныз қызақтырмалең осөз бір олас бірар аласын ақын адайтсың бар келаскі биді биді тұрсам барлап ақақақарғ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}